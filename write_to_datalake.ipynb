{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import fastparquet\n",
    "from io import StringIO\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import azure.functions as func\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.blob import BlobServiceClient, StandardBlobTier\n",
    "from azure.storage.filedatalake import DataLakeServiceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_blob_files(container_client, arg_date, std_date_format):\n",
    "    start_date = datetime.strptime(arg_date, std_date_format).date() - timedelta(days=1)\n",
    "\n",
    "    blob_files = [blob for blob in container_client.list_blobs() if blob.creation_time.date() >= start_date]\n",
    "\n",
    "    return blob_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_to_dataframe(container_client, filename, file_delimiter= ','):\n",
    "    blob_client = container_client.get_blob_client(blob=filename)\n",
    "\n",
    "    # Retrieve extract blob file\n",
    "    blob_download = blob_client.download_blob()\n",
    "\n",
    "    # Read blob file into DataFrame\n",
    "    blob_data = StringIO(blob_download.content_as_text())\n",
    "    df = pd.read_csv(blob_data,delimiter=file_delimiter)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframe_to_datalake(df, datalake_service_client, filesystem_name, dir_name, filename):\n",
    "\n",
    "    file_path = f'{dir_name}/{filename}'\n",
    "\n",
    "    file_client = datalake_service_client.get_file_client(filesystem_name, file_path)\n",
    "\n",
    "    processed_df = df.to_parquet(index=False)\n",
    "\n",
    "    file_client.upload_data(data=processed_df,overwrite=True, length=len(processed_df))\n",
    "\n",
    "    file_client.flush_data(len(processed_df))\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def archive_cooltier_blob_file(blob_service_client, storage_account_url, source_container, archive_container, blob_list):\n",
    "\n",
    "    for blob in blob_list:\n",
    "        blob_name = blob.name\n",
    "        source_blob_url = f'{storage_account_url}{source_container}/{blob_name}'\n",
    "\n",
    "        # Copy source blob file to archive container and change blob access tier to 'Cool'\n",
    "        archive_blob_client = blob_service_client.get_blob_client(archive_container, blob_name)\n",
    "\n",
    "        archive_blob_client.start_copy_from_url(source_url=source_blob_url, standard_blob_tier=StandardBlobTier.Cool)\n",
    "\n",
    "        (blob_service_client.get_blob_client(source_container, blob_name)).delete_blob(delete_snapshots='include')\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_relational_data(container_client, blob_file_list):\n",
    "    df = pd.concat([read_csv_to_dataframe(container_client=container_client, filename=blob_name.name) for blob_name in blob_file_list], ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_relational_data(df, columns, groupby_columns):\n",
    "    # Remove leading and trailing whitespace in df column names\n",
    "    processed_df = df.rename(columns=lambda x: x.strip())\n",
    "\n",
    "    # Filter DataFrame (df) columns\n",
    "    processed_df = processed_df.loc[:, columns]\n",
    "\n",
    "    # Clean column names for easy consumption\n",
    "    processed_df.columns = processed_df.columns.str.strip()\n",
    "    processed_df.columns = processed_df.columns.str.lower()\n",
    "    processed_df.columns = processed_df.columns.str.replace(' ', '_')\n",
    "    processed_df.columns = processed_df.columns.str.replace('(', '')\n",
    "    processed_df.columns = processed_df.columns.str.replace(')', '')\n",
    "\n",
    "    # Filter out all empty rows, if they exist.\n",
    "    processed_df.dropna(inplace=True)\n",
    "\n",
    "    # Remove leading and trailing whitespace for all string values in df\n",
    "    df_obj_cols = processed_df.select_dtypes(['object'])\n",
    "    processed_df[df_obj_cols.columns] = df_obj_cols.apply(lambda x: x.str.strip())\n",
    "\n",
    "    # Convert column to datetime: attempt to infer date format, return NA where conversion fails.\n",
    "    processed_df['date'] = pd.to_datetime( processed_df['date'], infer_datetime_format=True, errors='coerce')\n",
    "\n",
    "    # Convert object/string to numeric and handle special characters for each currency column\n",
    "    processed_df['gross_sales'] = processed_df['gross_sales'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "\n",
    "    # Capture dateparts (year and month) in new DataFrame columns\n",
    "    processed_df['sale_year'] = pd.DatetimeIndex(processed_df['date']).year\n",
    "    processed_df['sale_month'] = pd.DatetimeIndex(processed_df['date']).month\n",
    "\n",
    "    # Get Gross Sales per Segment, Country, Sale Year, and Sale Month\n",
    "    processed_df = processed_df.sort_values(by=['sale_year', 'sale_month']).groupby(groupby_columns, as_index=False).agg(total_units_sold=('units_sold', sum), total_gross_sales=('gross_sales', sum))\n",
    "\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_relational_data(processed_df, datalake_service_client, filesystem_name, dir_name, file_format, file_prefix):\n",
    "    now = datetime.today().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    processed_filename = f'{file_prefix}_{now}.{file_format}'\n",
    "    write_dataframe_to_datalake(processed_df, datalake_service_client, filesystem_name, dir_name, processed_filename)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cloud_etl(service_client, storage_account_url, source_container, archive_container, source_container_client, blob_file_list, columns, groupby_columns, datalake_service_client, filesystem_name, dir_name, file_format, file_prefix):\n",
    "    df = ingest_relational_data(source_container_client, blob_file_list)\n",
    "    df = process_relational_data(df, columns, groupby_columns)\n",
    "    result = load_relational_data(df, datalake_service_client, filesystem_name, dir_name, file_format, file_prefix)\n",
    "    result = archive_cooltier_blob_file(service_client, storage_account_url, source_container, archive_container, blob_file_list)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(req: func.HttpRequest) -> func.HttpResponse:\n",
    "    logging.info('Python HTTP trigger function processed a request.')\n",
    "\n",
    "    # Parameters/Configurations\n",
    "    arg_date = '2014-07-01'\n",
    "    std_date_format = '%Y-%m-%d'\n",
    "    processed_file_format = 'parquet'\n",
    "    processed_file_prefix = 'financial_demo'\n",
    "\n",
    "    # List of columns relevant for analysis\n",
    "    cols = ['Segment', 'Country', 'Units Sold', 'Gross Sales', 'Date']\n",
    "\n",
    "    # List of columns to aggregate\n",
    "    groupby_cols = ['segment', 'country', 'sale_year', 'sale_month']\n",
    "\n",
    "    try:\n",
    "        # Set variables from appsettings configurations/Environment Variables.\n",
    "        key_vault_name = os.environ[\"KEY_VAULT_NAME\"]\n",
    "        key_vault_Uri = f\"https://{key_vault_name}.vault.azure.net\"\n",
    "        blob_secret_name = os.environ[\"ABS_SECRET_NAME\"]\n",
    "\n",
    "        abs_acct_name='stcloudetldemodata'\n",
    "        abs_acct_url=f'https://{abs_acct_name}.blob.core.windows.net/'\n",
    "        abs_container_name='demo-cloudetl-data'\n",
    "        archive_container_name = 'demo-cloudetl-archive'\n",
    "\n",
    "        adls_acct_name='dlscloudetldemo'\n",
    "        adls_acct_url = f'https://{adls_acct_name}.dfs.core.windows.net/'\n",
    "        adls_fsys_name='processed-data-demo'\n",
    "        adls_dir_name='finance_data'\n",
    "        adls_secret_name='adls-access-key1'\n",
    "\n",
    "        # Authenticate and securely retrieve Key Vault secret for access key value.\n",
    "        az_credential = DefaultAzureCredential()\n",
    "        secret_client = SecretClient(vault_url=key_vault_Uri, credential= az_credential)\n",
    "        access_key_secret = secret_client.get_secret(blob_secret_name)\n",
    "\n",
    "        # Initialize Azure Service SDK Clients\n",
    "        abs_service_client = BlobServiceClient(\n",
    "            account_url = abs_acct_url,\n",
    "            credential = az_credential\n",
    "        )\n",
    "\n",
    "        abs_container_client = abs_service_client.get_container_client(container=abs_container_name)\n",
    "\n",
    "        adls_service_client = DataLakeServiceClient(\n",
    "            account_url = adls_acct_url,\n",
    "            credential = az_credential\n",
    "        )\n",
    "\n",
    "        # Run ETL Application\n",
    "        process_file_list = return_blob_files(\n",
    "            container_client = abs_container_client,\n",
    "            arg_date = arg_date,\n",
    "            std_date_format = std_date_format\n",
    "        )\n",
    "\n",
    "        run_cloud_etl(\n",
    "            source_container_client = abs_container_client,\n",
    "            blob_file_list = process_file_list,\n",
    "            columns = cols,\n",
    "            groupby_columns = groupby_cols,\n",
    "            datalake_service_client = adls_service_client,\n",
    "            filesystem_name = adls_fsys_name,\n",
    "            dir_name = adls_dir_name,\n",
    "            file_format = processed_file_format,\n",
    "            file_prefix = processed_file_prefix,\n",
    "            service_client = abs_service_client,\n",
    "            storage_account_url = abs_acct_url,\n",
    "            source_container = abs_container_name,\n",
    "            archive_container = archive_container_name\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.info(e)\n",
    "\n",
    "        return func.HttpResponse(\n",
    "                f\"!! This HTTP triggered function executed unsuccessfully. \\n\\t {e} \",\n",
    "                status_code=200\n",
    "        )\n",
    "\n",
    "    return func.HttpResponse(\"This HTTP triggered function executed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
